{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP - Experiment NER.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "BimukLqVlbDG"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/ENS/NLP/project"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSDoukiKmtjA",
        "outputId": "1decb155-ec90-4e36-b35b-544b079f5b89"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ENS/NLP/project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing requirements modules"
      ],
      "metadata": {
        "id": "BimukLqVlbDG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSf8Zychk7TA",
        "outputId": "fb7250eb-3ee4-482d-8d4c-226b403c5cfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\n",
            "\u001b[K     |████████████████████████████████| 325 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 61.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 2)) (4.63.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 54.9 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 2)) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 2)) (1.21.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 2)) (4.11.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 2)) (0.3.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 2)) (21.3)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 48.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 2)) (0.70.12.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 2)) (2.23.0)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 51.6 MB/s \n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets->-r requirements.txt (line 2)) (3.10.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets->-r requirements.txt (line 2)) (3.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets->-r requirements.txt (line 2)) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets->-r requirements.txt (line 2)) (3.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 2)) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 2)) (3.0.4)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 67.4 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 55.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 3)) (2019.12.20)\n",
            "Collecting pyyaml\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 43.8 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 53.3 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 60.7 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (21.4.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 57.7 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (2.0.12)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets->-r requirements.txt (line 2)) (3.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets->-r requirements.txt (line 2)) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets->-r requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->-r requirements.txt (line 3)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->-r requirements.txt (line 3)) (1.1.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, pyyaml, fsspec, aiohttp, xxhash, tokenizers, sacremoses, responses, huggingface-hub, transformers, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.0.0 frozenlist-1.3.0 fsspec-2022.3.0 huggingface-hub-0.4.0 multidict-6.0.2 pyyaml-6.0 responses-0.18.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.17.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 1 : CANINE-s vs CANINE-c\n",
        "\n",
        "Checking impact of training "
      ],
      "metadata": {
        "id": "lmGlGT4gnnkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 main.py --model_type CANINE --canine_mode c --mode EVAL --train_lang ES --eval_lang ES --weight_name CANINE-C_lr_2e-05_val_loss_0.01858_ep_4.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEXenH5inn0J",
        "outputId": "87dd911c-bea4-4b1a-d8d6-504f53e33c6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ Creating model  CANINE ¤¤¤¤¤\n",
            "Some weights of CanineForTokenClassification were not initialized from the model checkpoint at google/canine-c and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : Espagnol version ¤¤¤¤¤\n",
            "Reusing dataset conll2002 (/root/.cache/huggingface/datasets/conll2002/es/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5)\n",
            "100% 3/3 [00:00<00:00, 630.79it/s]\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : Dutch version ¤¤¤¤¤\n",
            "Reusing dataset conll2002 (/root/.cache/huggingface/datasets/conll2002/nl/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5)\n",
            "100% 3/3 [00:00<00:00, 739.87it/s]\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : English version ¤¤¤¤¤\n",
            "Reusing dataset conll2003 (/root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee)\n",
            "100% 3/3 [00:00<00:00, 639.73it/s]\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CANINE pretrained weights on NER task ¤¤¤¤¤\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ Start evaluation with  ES  dataset ¤¤¤¤¤\n",
            "100% 1.52k/1.52k [01:25<00:00, 17.7it/s, F1_score=0.946]\n",
            "final F1 score is :  0.9458554524320564\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 main.py --model_type CANINE --canine_mode s --mode FULL --train_lang ES --eval_lang ES"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lshi1UYopwWO",
        "outputId": "c9c40cec-19c6-40aa-9c9a-08acdf5b3a27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ Creating model  CANINE ¤¤¤¤¤\n",
            "Some weights of CanineForTokenClassification were not initialized from the model checkpoint at google/canine-c and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : Espagnol version ¤¤¤¤¤\n",
            "Reusing dataset conll2002 (/root/.cache/huggingface/datasets/conll2002/es/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5)\n",
            "100% 3/3 [00:00<00:00, 676.94it/s]\n",
            "Downloading: 100% 657/657 [00:00<00:00, 611kB/s]\n",
            "Downloading: 100% 854/854 [00:00<00:00, 898kB/s]\n",
            "Downloading: 100% 670/670 [00:00<00:00, 752kB/s]\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : Dutch version ¤¤¤¤¤\n",
            "Reusing dataset conll2002 (/root/.cache/huggingface/datasets/conll2002/nl/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5)\n",
            "100% 3/3 [00:00<00:00, 824.73it/s]\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : English version ¤¤¤¤¤\n",
            "Reusing dataset conll2003 (/root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee)\n",
            "100% 3/3 [00:00<00:00, 667.49it/s]\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ Start training with ES  dataset ¤¤¤¤¤\n",
            " 10% 831/8323 [03:01<26:34,  4.70it/s]\n",
            "Iteration 832/8323 of epoch 1 complete. Loss : 0.02555333822965622\n",
            " 20% 1663/8323 [06:01<22:41,  4.89it/s]\n",
            "Iteration 1664/8323 of epoch 1 complete. Loss : 0.01232913974672556\n",
            " 30% 2495/8323 [09:00<22:36,  4.30it/s]\n",
            "Iteration 2496/8323 of epoch 1 complete. Loss : 6.681687955278903e-05\n",
            " 40% 3327/8323 [12:00<16:45,  4.97it/s]\n",
            "Iteration 3328/8323 of epoch 1 complete. Loss : 0.0006022438174113631\n",
            " 50% 4159/8323 [15:00<14:39,  4.73it/s]\n",
            "Iteration 4160/8323 of epoch 1 complete. Loss : 0.0007484758971258998\n",
            " 60% 4991/8323 [18:00<12:05,  4.59it/s]\n",
            "Iteration 4992/8323 of epoch 1 complete. Loss : 0.07462985813617706\n",
            " 70% 5823/8323 [21:00<08:44,  4.77it/s]\n",
            "Iteration 5824/8323 of epoch 1 complete. Loss : 0.105289526283741\n",
            " 80% 6655/8323 [24:00<05:30,  5.04it/s]\n",
            "Iteration 6656/8323 of epoch 1 complete. Loss : 0.014934562146663666\n",
            " 90% 7487/8323 [27:01<03:06,  4.48it/s]\n",
            "Iteration 7488/8323 of epoch 1 complete. Loss : 0.07449112087488174\n",
            "100% 8319/8323 [30:01<00:00,  4.67it/s]\n",
            "Iteration 8320/8323 of epoch 1 complete. Loss : 0.05810292810201645\n",
            "100% 8323/8323 [30:02<00:00,  4.62it/s]\n",
            "100% 1915/1915 [01:44<00:00, 18.31it/s]\n",
            "\n",
            "Epoch 1 complete! Validation Loss : 0.02852840516623915\n",
            "Best validation loss improved from inf to 0.02852840516623915\n",
            "\n",
            " 10% 831/8323 [03:00<25:54,  4.82it/s]\n",
            "Iteration 832/8323 of epoch 2 complete. Loss : 0.019158929586410522\n",
            " 20% 1663/8323 [06:00<23:03,  4.81it/s]\n",
            "Iteration 1664/8323 of epoch 2 complete. Loss : 0.007270489819347858\n",
            " 30% 2495/8323 [08:59<19:59,  4.86it/s]\n",
            "Iteration 2496/8323 of epoch 2 complete. Loss : 2.2859427190269344e-05\n",
            " 40% 3327/8323 [12:00<17:13,  4.83it/s]\n",
            "Iteration 3328/8323 of epoch 2 complete. Loss : 0.0001517681375844404\n",
            " 50% 4159/8323 [15:00<15:44,  4.41it/s]\n",
            "Iteration 4160/8323 of epoch 2 complete. Loss : 0.00036998215364292264\n",
            " 60% 4991/8323 [17:57<11:23,  4.88it/s]\n",
            "Iteration 4992/8323 of epoch 2 complete. Loss : 0.06228172034025192\n",
            " 70% 5823/8323 [20:55<08:52,  4.70it/s]\n",
            "Iteration 5824/8323 of epoch 2 complete. Loss : 0.09254654496908188\n",
            " 80% 6655/8323 [23:53<05:24,  5.14it/s]\n",
            "Iteration 6656/8323 of epoch 2 complete. Loss : 0.010189513675868511\n",
            " 90% 7487/8323 [26:52<02:48,  4.95it/s]\n",
            "Iteration 7488/8323 of epoch 2 complete. Loss : 0.05780980363488197\n",
            "100% 8319/8323 [29:51<00:00,  4.54it/s]\n",
            "Iteration 8320/8323 of epoch 2 complete. Loss : 0.05368227884173393\n",
            "100% 8323/8323 [29:52<00:00,  4.64it/s]\n",
            "100% 1915/1915 [01:43<00:00, 18.55it/s]\n",
            "\n",
            "Epoch 2 complete! Validation Loss : 0.025202143855530653\n",
            "Best validation loss improved from 0.02852840516623915 to 0.025202143855530653\n",
            "\n",
            " 10% 831/8323 [02:59<27:22,  4.56it/s]\n",
            "Iteration 832/8323 of epoch 3 complete. Loss : 0.02161666564643383\n",
            " 20% 1663/8323 [06:00<23:35,  4.71it/s]\n",
            "Iteration 1664/8323 of epoch 3 complete. Loss : 0.005511026363819838\n",
            " 30% 2495/8323 [08:58<21:22,  4.54it/s]\n",
            "Iteration 2496/8323 of epoch 3 complete. Loss : 1.0622113222780172e-05\n",
            " 40% 3327/8323 [11:56<17:58,  4.63it/s]\n",
            "Iteration 3328/8323 of epoch 3 complete. Loss : 0.00011832870222860947\n",
            " 50% 4159/8323 [14:55<14:54,  4.65it/s]\n",
            "Iteration 4160/8323 of epoch 3 complete. Loss : 0.00020209516515024006\n",
            " 60% 4991/8323 [17:54<11:07,  5.00it/s]\n",
            "Iteration 4992/8323 of epoch 3 complete. Loss : 0.05201716348528862\n",
            " 70% 5823/8323 [20:54<09:41,  4.30it/s]\n",
            "Iteration 5824/8323 of epoch 3 complete. Loss : 0.07835227251052856\n",
            " 80% 6655/8323 [23:53<06:01,  4.61it/s]\n",
            "Iteration 6656/8323 of epoch 3 complete. Loss : 0.005199463572353125\n",
            " 90% 7487/8323 [26:52<02:43,  5.12it/s]\n",
            "Iteration 7488/8323 of epoch 3 complete. Loss : 0.05141609534621239\n",
            "100% 8319/8323 [29:51<00:00,  4.48it/s]\n",
            "Iteration 8320/8323 of epoch 3 complete. Loss : 0.043997377157211304\n",
            "100% 8323/8323 [29:52<00:00,  4.64it/s]\n",
            "100% 1915/1915 [01:43<00:00, 18.56it/s]\n",
            "\n",
            "Epoch 3 complete! Validation Loss : 0.022916083255444624\n",
            "Best validation loss improved from 0.025202143855530653 to 0.022916083255444624\n",
            "\n",
            " 10% 831/8323 [02:59<27:19,  4.57it/s]\n",
            "Iteration 832/8323 of epoch 4 complete. Loss : 0.02807900868356228\n",
            " 20% 1663/8323 [05:58<23:46,  4.67it/s]\n",
            "Iteration 1664/8323 of epoch 4 complete. Loss : 0.0032021955121308565\n",
            " 30% 2495/8323 [08:57<20:39,  4.70it/s]\n",
            "Iteration 2496/8323 of epoch 4 complete. Loss : 7.165094757510815e-06\n",
            " 40% 3327/8323 [11:57<19:29,  4.27it/s]\n",
            "Iteration 3328/8323 of epoch 4 complete. Loss : 9.225035319104791e-05\n",
            " 50% 4159/8323 [14:56<14:26,  4.80it/s]\n",
            "Iteration 4160/8323 of epoch 4 complete. Loss : 0.00017040662351064384\n",
            " 60% 4991/8323 [17:56<11:21,  4.89it/s]\n",
            "Iteration 4992/8323 of epoch 4 complete. Loss : 0.045795511454343796\n",
            " 70% 5823/8323 [20:56<09:01,  4.62it/s]\n",
            "Iteration 5824/8323 of epoch 4 complete. Loss : 0.063482366502285\n",
            " 80% 6655/8323 [23:56<06:28,  4.29it/s]\n",
            "Iteration 6656/8323 of epoch 4 complete. Loss : 0.007304633501917124\n",
            " 90% 7487/8323 [26:55<03:01,  4.59it/s]\n",
            "Iteration 7488/8323 of epoch 4 complete. Loss : 0.028624756261706352\n",
            "100% 8319/8323 [29:54<00:00,  4.61it/s]\n",
            "Iteration 8320/8323 of epoch 4 complete. Loss : 0.04123830422759056\n",
            "100% 8323/8323 [29:55<00:00,  4.63it/s]\n",
            "100% 1915/1915 [01:43<00:00, 18.53it/s]\n",
            "\n",
            "Epoch 4 complete! Validation Loss : 0.02011890066451859\n",
            "Best validation loss improved from 0.022916083255444624 to 0.02011890066451859\n",
            "\n",
            "The model has been saved in /content/drive/MyDrive/ENS/NLP/models/CANINE_lr_2e-05_val_loss_0.02012_ep_4.pt\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ Start evaluation with  ES  dataset ¤¤¤¤¤\n",
            "100% 1.52k/1.52k [01:26<00:00, 17.6it/s, F1_score=0.946]\n",
            "final F1 score is :  0.9455407797526652\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 main.py --model_type CANINE --canine_mode c --mode EVAL --eval_lang EN --weight_name CANINE-C_EN_lr_2e-05_val_loss_0.01035_ep_4.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Npl8GGX1ZACZ",
        "outputId": "f25f6767-b3c6-484b-a017-8538d233894c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ Creating model  CANINE ¤¤¤¤¤\n",
            "Downloading: 100% 698/698 [00:00<00:00, 1.02MB/s]\n",
            "Downloading: 100% 504M/504M [00:07<00:00, 67.8MB/s]\n",
            "Some weights of CanineForTokenClassification were not initialized from the model checkpoint at google/canine-c and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : Espagnol version ¤¤¤¤¤\n",
            "Downloading builder script: 9.23kB [00:00, 11.1MB/s]       \n",
            "Downloading metadata: 7.46kB [00:00, 11.1MB/s]       \n",
            "Downloading and preparing dataset conll2002/es (download: 3.95 MiB, generated: 8.87 MiB, post-processed: Unknown size, total: 12.82 MiB) to /root/.cache/huggingface/datasets/conll2002/es/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5...\n",
            "Downloading data files:   0% 0/3 [00:00<?, ?it/s]\n",
            "Downloading data: 2.97MB [00:00, 73.0MB/s]      \n",
            "Downloading data files:  33% 1/3 [00:00<00:01,  1.24it/s]\n",
            "Downloading data: 594kB [00:00, 41.7MB/s]       \n",
            "Downloading data files:  67% 2/3 [00:01<00:00,  1.30it/s]\n",
            "Downloading data: 576kB [00:00, 38.6MB/s]       \n",
            "Downloading data files: 100% 3/3 [00:02<00:00,  1.37it/s]\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 1704.77it/s]\n",
            "Dataset conll2002 downloaded and prepared to /root/.cache/huggingface/datasets/conll2002/es/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 389.37it/s]\n",
            "Downloading: 100% 657/657 [00:00<00:00, 1.04MB/s]\n",
            "Downloading: 100% 892/892 [00:00<00:00, 1.58MB/s]\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : Dutch version ¤¤¤¤¤\n",
            "Downloading and preparing dataset conll2002/nl (download: 3.47 MiB, generated: 7.74 MiB, post-processed: Unknown size, total: 11.21 MiB) to /root/.cache/huggingface/datasets/conll2002/nl/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5...\n",
            "Downloading data files:   0% 0/3 [00:00<?, ?it/s]\n",
            "Downloading data: 2.38MB [00:00, 63.7MB/s]      \n",
            "Downloading data files:  33% 1/3 [00:00<00:00,  2.47it/s]\n",
            "Downloading data: 451kB [00:00, 34.7MB/s]       \n",
            "Downloading data files:  67% 2/3 [00:00<00:00,  3.24it/s]\n",
            "Downloading data: 814kB [00:00, 38.4MB/s]       \n",
            "Downloading data files: 100% 3/3 [00:01<00:00,  2.57it/s]\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 1752.25it/s]\n",
            "Dataset conll2002 downloaded and prepared to /root/.cache/huggingface/datasets/conll2002/nl/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 828.37it/s]\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : English version ¤¤¤¤¤\n",
            "Downloading builder script: 9.52kB [00:00, 11.2MB/s]       \n",
            "Downloading metadata: 3.79kB [00:00, 5.50MB/s]       \n",
            "Downloading and preparing dataset conll2003/conll2003 (download: 959.94 KiB, generated: 9.78 MiB, post-processed: Unknown size, total: 10.72 MiB) to /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee...\n",
            "Downloading data: 100% 983k/983k [00:00<00:00, 1.38MB/s]\n",
            "Dataset conll2003 downloaded and prepared to /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 548.92it/s]\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CANINE pretrained weights on NER task ¤¤¤¤¤\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ Start evaluation with  EN  dataset ¤¤¤¤¤\n",
            "100% 3.45k/3.45k [03:09<00:00, 18.3it/s, F1_score=0.914]\n",
            "final F1 score is :  0.9139266548859056\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 main.py --model_type CANINE --canine_mode c --mode FULL --train_lang NL --eval_lang NL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJHw4mlqBjSh",
        "outputId": "d79b2199-98a8-43b9-ca02-a25c1b809d5d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ Creating model  CANINE ¤¤¤¤¤\n",
            "Some weights of CanineForTokenClassification were not initialized from the model checkpoint at google/canine-c and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : Espagnol version ¤¤¤¤¤\n",
            "Reusing dataset conll2002 (/root/.cache/huggingface/datasets/conll2002/es/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5)\n",
            "100% 3/3 [00:00<00:00, 629.24it/s]\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : Dutch version ¤¤¤¤¤\n",
            "Reusing dataset conll2002 (/root/.cache/huggingface/datasets/conll2002/nl/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5)\n",
            "100% 3/3 [00:00<00:00, 794.13it/s]\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : English version ¤¤¤¤¤\n",
            "Reusing dataset conll2003 (/root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee)\n",
            "100% 3/3 [00:00<00:00, 571.79it/s]\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ Start training with NL  dataset ¤¤¤¤¤\n",
            " 10% 1579/15806 [05:20<49:44,  4.77it/s]\n",
            "Iteration 1580/15806 of epoch 1 complete. Loss : 0.02016785368323326\n",
            " 20% 3159/15806 [10:39<40:26,  5.21it/s]\n",
            "Iteration 3160/15806 of epoch 1 complete. Loss : 0.00022005537175573409\n",
            " 30% 4739/15806 [16:00<37:42,  4.89it/s]\n",
            "Iteration 4740/15806 of epoch 1 complete. Loss : 0.00018213395378552377\n",
            " 40% 6319/15806 [21:20<32:01,  4.94it/s]\n",
            "Iteration 6320/15806 of epoch 1 complete. Loss : 0.009477358311414719\n",
            " 50% 7899/15806 [26:40<26:20,  5.00it/s]\n",
            "Iteration 7900/15806 of epoch 1 complete. Loss : 0.00014095826190896332\n",
            " 60% 9479/15806 [32:00<23:14,  4.54it/s]\n",
            "Iteration 9480/15806 of epoch 1 complete. Loss : 0.05526946112513542\n",
            " 70% 11059/15806 [37:19<17:59,  4.40it/s]\n",
            "Iteration 11060/15806 of epoch 1 complete. Loss : 0.049623776227235794\n",
            " 80% 12639/15806 [42:39<09:57,  5.30it/s]\n",
            "Iteration 12640/15806 of epoch 1 complete. Loss : 0.015821227803826332\n",
            " 90% 14219/15806 [47:59<05:02,  5.25it/s]\n",
            "Iteration 14220/15806 of epoch 1 complete. Loss : 0.000696650065947324\n",
            "100% 15799/15806 [53:19<00:01,  5.16it/s]\n",
            "Iteration 15800/15806 of epoch 1 complete. Loss : 0.007142303977161646\n",
            "100% 15806/15806 [53:21<00:00,  4.94it/s]\n",
            "100% 2895/2895 [02:25<00:00, 19.90it/s]\n",
            "\n",
            "Epoch 1 complete! Validation Loss : 0.012416631971172607\n",
            "Best validation loss improved from inf to 0.012416631971172607\n",
            "\n",
            " 10% 1579/15806 [05:22<46:02,  5.15it/s]\n",
            "Iteration 1580/15806 of epoch 2 complete. Loss : 0.00739213777706027\n",
            " 20% 3159/15806 [10:43<39:58,  5.27it/s]\n",
            "Iteration 3160/15806 of epoch 2 complete. Loss : 0.001084810122847557\n",
            " 30% 4739/15806 [16:04<37:08,  4.97it/s]\n",
            "Iteration 4740/15806 of epoch 2 complete. Loss : 3.84215236408636e-05\n",
            " 40% 6319/15806 [21:25<32:19,  4.89it/s]\n",
            "Iteration 6320/15806 of epoch 2 complete. Loss : 0.004072069656103849\n",
            " 50% 7899/15806 [26:46<27:46,  4.75it/s]\n",
            "Iteration 7900/15806 of epoch 2 complete. Loss : 0.0003650508006103337\n",
            " 60% 9479/15806 [32:06<23:24,  4.50it/s]\n",
            "Iteration 9480/15806 of epoch 2 complete. Loss : 0.053316835314035416\n",
            " 70% 11059/15806 [37:28<15:21,  5.15it/s]\n",
            "Iteration 11060/15806 of epoch 2 complete. Loss : 0.04754799231886864\n",
            " 80% 12639/15806 [42:49<09:47,  5.39it/s]\n",
            "Iteration 12640/15806 of epoch 2 complete. Loss : 0.014201612211763859\n",
            " 90% 14219/15806 [48:09<05:14,  5.05it/s]\n",
            "Iteration 14220/15806 of epoch 2 complete. Loss : 0.00020326722005847842\n",
            "100% 15799/15806 [53:30<00:01,  4.20it/s]\n",
            "Iteration 15800/15806 of epoch 2 complete. Loss : 0.005565549246966839\n",
            "100% 15806/15806 [53:32<00:00,  4.92it/s]\n",
            "100% 2895/2895 [02:24<00:00, 20.01it/s]\n",
            "\n",
            "Epoch 2 complete! Validation Loss : 0.011826706005743625\n",
            "Best validation loss improved from 0.012416631971172607 to 0.011826706005743625\n",
            "\n",
            " 10% 1579/15806 [05:21<44:29,  5.33it/s]\n",
            "Iteration 1580/15806 of epoch 3 complete. Loss : 0.0057778931222856045\n",
            " 20% 3159/15806 [10:42<41:02,  5.14it/s]\n",
            "Iteration 3160/15806 of epoch 3 complete. Loss : 3.918597940355539e-05\n",
            " 30% 4739/15806 [16:08<37:47,  4.88it/s]\n",
            "Iteration 4740/15806 of epoch 3 complete. Loss : 4.6580938942497596e-05\n",
            " 40% 6319/15806 [21:31<32:52,  4.81it/s]\n",
            "Iteration 6320/15806 of epoch 3 complete. Loss : 0.001146697555668652\n",
            " 50% 7899/15806 [26:53<25:14,  5.22it/s]\n",
            "Iteration 7900/15806 of epoch 3 complete. Loss : 2.5691922928672284e-05\n",
            " 60% 9479/15806 [32:16<21:11,  4.97it/s]\n",
            "Iteration 9480/15806 of epoch 3 complete. Loss : 0.05128781870007515\n",
            " 70% 11059/15806 [37:38<16:16,  4.86it/s]\n",
            "Iteration 11060/15806 of epoch 3 complete. Loss : 0.056769099086523056\n",
            " 80% 12639/15806 [43:01<12:17,  4.29it/s]\n",
            "Iteration 12640/15806 of epoch 3 complete. Loss : 0.008194555528461933\n",
            " 90% 14219/15806 [48:25<05:53,  4.49it/s]\n",
            "Iteration 14220/15806 of epoch 3 complete. Loss : 0.0001299656869377941\n",
            "100% 15799/15806 [53:48<00:01,  4.99it/s]\n",
            "Iteration 15800/15806 of epoch 3 complete. Loss : 0.0077522327192127705\n",
            "100% 15806/15806 [53:49<00:00,  4.89it/s]\n",
            "100% 2895/2895 [02:25<00:00, 19.84it/s]\n",
            "\n",
            "Epoch 3 complete! Validation Loss : 0.010123690138387825\n",
            "Best validation loss improved from 0.011826706005743625 to 0.010123690138387825\n",
            "\n",
            " 10% 1579/15806 [05:23<56:13,  4.22it/s]\n",
            "Iteration 1580/15806 of epoch 4 complete. Loss : 0.004731170367449522\n",
            " 20% 3159/15806 [10:45<41:39,  5.06it/s]\n",
            "Iteration 3160/15806 of epoch 4 complete. Loss : 7.883542275521904e-05\n",
            " 30% 4739/15806 [16:08<36:01,  5.12it/s]\n",
            "Iteration 4740/15806 of epoch 4 complete. Loss : 8.707745109859388e-06\n",
            " 40% 6319/15806 [21:32<31:35,  5.00it/s]\n",
            "Iteration 6320/15806 of epoch 4 complete. Loss : 0.00030311563750728965\n",
            " 50% 7899/15806 [26:54<26:55,  4.90it/s]\n",
            "Iteration 7900/15806 of epoch 4 complete. Loss : 2.450631836836692e-05\n",
            " 60% 9479/15806 [32:17<21:31,  4.90it/s]\n",
            "Iteration 9480/15806 of epoch 4 complete. Loss : 0.04895688220858574\n",
            " 70% 11059/15806 [37:38<16:50,  4.70it/s]\n",
            "Iteration 11060/15806 of epoch 4 complete. Loss : 0.053167834877967834\n",
            " 80% 12639/15806 [42:57<11:08,  4.73it/s]\n",
            "Iteration 12640/15806 of epoch 4 complete. Loss : 0.004560047294944525\n",
            " 90% 14219/15806 [48:14<06:04,  4.35it/s]\n",
            "Iteration 14220/15806 of epoch 4 complete. Loss : 3.4662356483750045e-05\n",
            "100% 15799/15806 [53:31<00:01,  5.35it/s]\n",
            "Iteration 15800/15806 of epoch 4 complete. Loss : 0.0031987426336854696\n",
            "100% 15806/15806 [53:33<00:00,  4.92it/s]\n",
            "100% 2895/2895 [02:22<00:00, 20.28it/s]\n",
            "\n",
            "Epoch 4 complete! Validation Loss : 0.008974896922857597\n",
            "Best validation loss improved from 0.010123690138387825 to 0.008974896922857597\n",
            "\n",
            "The model has been saved in /content/drive/MyDrive/ENS/NLP/models/CANINE_lr_2e-05_val_loss_0.00897_ep_4.pt\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ Start evaluation with  NL  dataset ¤¤¤¤¤\n",
            "100% 5.20k/5.20k [06:00<00:00, 14.4it/s, F1_score=0.951]\n",
            "final F1 score is :  0.950898116310144\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 main.py --model_type CANINE --canine_mode s --mode FULL --train_lang EN --eval_lang EN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rtmc2zF4B_t7",
        "outputId": "9dc2b070-77d7-47eb-94b7-04c1fb0dc68d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ Creating model  CANINE ¤¤¤¤¤\n",
            "Some weights of CanineForTokenClassification were not initialized from the model checkpoint at google/canine-c and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : Espagnol version ¤¤¤¤¤\n",
            "Reusing dataset conll2002 (/root/.cache/huggingface/datasets/conll2002/es/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5)\n",
            "100% 3/3 [00:00<00:00, 640.32it/s]\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : Dutch version ¤¤¤¤¤\n",
            "Reusing dataset conll2002 (/root/.cache/huggingface/datasets/conll2002/nl/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5)\n",
            "100% 3/3 [00:00<00:00, 802.23it/s]\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : English version ¤¤¤¤¤\n",
            "Reusing dataset conll2003 (/root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee)\n",
            "100% 3/3 [00:00<00:00, 576.83it/s]\n",
            "Downloading: 100% 657/657 [00:00<00:00, 642kB/s]\n",
            "Downloading: 100% 854/854 [00:00<00:00, 798kB/s]\n",
            "Downloading: 100% 670/670 [00:00<00:00, 635kB/s]\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ Start training with EN  dataset ¤¤¤¤¤\n",
            " 10% 1403/14041 [04:52<41:01,  5.13it/s]\n",
            "Iteration 1404/14041 of epoch 1 complete. Loss : 0.13372696936130524\n",
            " 20% 2807/14041 [09:54<38:29,  4.87it/s]\n",
            "Iteration 2808/14041 of epoch 1 complete. Loss : 0.004623385611921549\n",
            " 30% 4211/14041 [14:55<36:04,  4.54it/s]\n",
            "Iteration 4212/14041 of epoch 1 complete. Loss : 0.05411195382475853\n",
            " 40% 5615/14041 [20:01<29:07,  4.82it/s]\n",
            "Iteration 5616/14041 of epoch 1 complete. Loss : 0.009634304791688919\n",
            " 50% 7019/14041 [25:04<25:19,  4.62it/s]\n",
            "Iteration 7020/14041 of epoch 1 complete. Loss : 0.027382979169487953\n",
            " 60% 8423/14041 [30:06<17:43,  5.28it/s]\n",
            "Iteration 8424/14041 of epoch 1 complete. Loss : 0.017483344301581383\n",
            " 70% 9827/14041 [35:13<14:52,  4.72it/s]\n",
            "Iteration 9828/14041 of epoch 1 complete. Loss : 0.05774705484509468\n",
            " 80% 11231/14041 [40:20<09:46,  4.79it/s]\n",
            "Iteration 11232/14041 of epoch 1 complete. Loss : 0.0012494103284552693\n",
            " 90% 12635/14041 [45:27<04:49,  4.86it/s]\n",
            "Iteration 12636/14041 of epoch 1 complete. Loss : 0.010658646933734417\n",
            "100% 14039/14041 [50:37<00:00,  4.70it/s]\n",
            "Iteration 14040/14041 of epoch 1 complete. Loss : 0.003196998266503215\n",
            "100% 14041/14041 [50:37<00:00,  4.62it/s]\n",
            "100% 3250/3250 [03:01<00:00, 17.89it/s]\n",
            "\n",
            "Epoch 1 complete! Validation Loss : 0.018924681218755162\n",
            "Best validation loss improved from inf to 0.018924681218755162\n",
            "\n",
            " 10% 1403/14041 [04:49<45:12,  4.66it/s]\n",
            "Iteration 1404/14041 of epoch 2 complete. Loss : 0.0822998434305191\n",
            " 20% 2807/14041 [09:45<38:39,  4.84it/s]\n",
            "Iteration 2808/14041 of epoch 2 complete. Loss : 0.004912371747195721\n",
            " 30% 4211/14041 [14:42<38:37,  4.24it/s]\n",
            "Iteration 4212/14041 of epoch 2 complete. Loss : 0.03143226355314255\n",
            " 40% 5615/14041 [19:30<34:19,  4.09it/s]\n",
            "Iteration 5616/14041 of epoch 2 complete. Loss : 0.005636798683553934\n",
            " 50% 7019/14041 [24:39<24:55,  4.69it/s]\n",
            "Iteration 7020/14041 of epoch 2 complete. Loss : 0.009743704460561275\n",
            " 60% 8423/14041 [29:45<18:33,  5.04it/s]\n",
            "Iteration 8424/14041 of epoch 2 complete. Loss : 0.010682754218578339\n",
            " 70% 9827/14041 [34:38<13:05,  5.36it/s]\n",
            "Iteration 9828/14041 of epoch 2 complete. Loss : 0.06193128973245621\n",
            " 80% 11231/14041 [39:28<10:44,  4.36it/s]\n",
            "Iteration 11232/14041 of epoch 2 complete. Loss : 0.0006450394866988063\n",
            " 90% 12635/14041 [44:19<05:31,  4.24it/s]\n",
            "Iteration 12636/14041 of epoch 2 complete. Loss : 0.005877200979739428\n",
            "100% 14039/14041 [49:22<00:00,  5.07it/s]\n",
            "Iteration 14040/14041 of epoch 2 complete. Loss : 0.002396515803411603\n",
            "100% 14041/14041 [49:22<00:00,  4.74it/s]\n",
            "100% 3250/3250 [03:02<00:00, 17.84it/s]\n",
            "\n",
            "Epoch 2 complete! Validation Loss : 0.01428078287246997\n",
            "Best validation loss improved from 0.018924681218755162 to 0.01428078287246997\n",
            "\n",
            " 10% 1403/14041 [04:58<41:08,  5.12it/s]\n",
            "Iteration 1404/14041 of epoch 3 complete. Loss : 0.06855195760726929\n",
            " 20% 2807/14041 [10:03<36:59,  5.06it/s]\n",
            "Iteration 2808/14041 of epoch 3 complete. Loss : 0.0043206349946558475\n",
            " 30% 4211/14041 [14:57<33:03,  4.96it/s]\n",
            "Iteration 4212/14041 of epoch 3 complete. Loss : 0.024452904239296913\n",
            " 40% 5615/14041 [19:48<33:51,  4.15it/s]\n",
            "Iteration 5616/14041 of epoch 3 complete. Loss : 0.004039011895656586\n",
            " 50% 7019/14041 [24:32<22:19,  5.24it/s]\n",
            "Iteration 7020/14041 of epoch 3 complete. Loss : 0.002661455888301134\n",
            " 60% 8423/14041 [29:25<17:46,  5.27it/s]\n",
            "Iteration 8424/14041 of epoch 3 complete. Loss : 0.006193554028868675\n",
            " 70% 9827/14041 [34:30<17:00,  4.13it/s]\n",
            "Iteration 9828/14041 of epoch 3 complete. Loss : 0.05911129340529442\n",
            " 80% 11231/14041 [39:30<08:44,  5.35it/s]\n",
            "Iteration 11232/14041 of epoch 3 complete. Loss : 5.455283826449886e-05\n",
            " 90% 12635/14041 [44:24<04:33,  5.14it/s]\n",
            "Iteration 12636/14041 of epoch 3 complete. Loss : 0.004644426982849836\n",
            "100% 14039/14041 [49:21<00:00,  4.85it/s]\n",
            "Iteration 14040/14041 of epoch 3 complete. Loss : 0.0005972635117359459\n",
            "100% 14041/14041 [49:22<00:00,  4.74it/s]\n",
            "100% 3250/3250 [03:01<00:00, 17.89it/s]\n",
            "\n",
            "Epoch 3 complete! Validation Loss : 0.012002711236364821\n",
            "Best validation loss improved from 0.01428078287246997 to 0.012002711236364821\n",
            "\n",
            " 10% 1403/14041 [04:43<41:24,  5.09it/s]\n",
            "Iteration 1404/14041 of epoch 4 complete. Loss : 0.05647948756814003\n",
            " 20% 2807/14041 [09:33<37:24,  5.01it/s]\n",
            "Iteration 2808/14041 of epoch 4 complete. Loss : 0.0030141258612275124\n",
            " 30% 4211/14041 [14:23<31:45,  5.16it/s]\n",
            "Iteration 4212/14041 of epoch 4 complete. Loss : 0.0230238139629364\n",
            " 40% 5615/14041 [19:21<33:37,  4.18it/s]\n",
            "Iteration 5616/14041 of epoch 4 complete. Loss : 0.0012216971954330802\n",
            " 50% 7019/14041 [24:28<25:24,  4.61it/s]\n",
            "Iteration 7020/14041 of epoch 4 complete. Loss : 0.00044857352622784674\n",
            " 60% 8423/14041 [29:18<17:35,  5.32it/s]\n",
            "Iteration 8424/14041 of epoch 4 complete. Loss : 0.002433852292597294\n",
            " 70% 9827/14041 [34:19<15:59,  4.39it/s]\n",
            "Iteration 9828/14041 of epoch 4 complete. Loss : 0.047854844480752945\n",
            " 80% 11231/14041 [39:22<08:42,  5.37it/s]\n",
            "Iteration 11232/14041 of epoch 4 complete. Loss : 1.8869472114602104e-05\n",
            " 90% 12635/14041 [44:20<04:22,  5.36it/s]\n",
            "Iteration 12636/14041 of epoch 4 complete. Loss : 0.0041198586113750935\n",
            "100% 14039/14041 [49:20<00:00,  5.09it/s]\n",
            "Iteration 14040/14041 of epoch 4 complete. Loss : 0.00022088810510467738\n",
            "100% 14041/14041 [49:21<00:00,  4.74it/s]\n",
            "100% 3250/3250 [03:01<00:00, 17.95it/s]\n",
            "\n",
            "Epoch 4 complete! Validation Loss : 0.010527662527454525\n",
            "Best validation loss improved from 0.012002711236364821 to 0.010527662527454525\n",
            "\n",
            "The model has been saved in /content/drive/MyDrive/ENS/NLP/models/CANINE_lr_2e-05_val_loss_0.01053_ep_4.pt\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ Start evaluation with  EN  dataset ¤¤¤¤¤\n",
            "100% 3.45k/3.45k [03:17<00:00, 17.4it/s, F1_score=0.913]\n",
            "final F1 score is :  0.9131821888371557\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 main.py --model_type CANINE --canine_mode s --mode FULL --train_lang NL --eval_lang NL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGc6_56UCBo2",
        "outputId": "f47576bc-461d-423c-babb-874ee45f4d86"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ Creating model  CANINE ¤¤¤¤¤\n",
            "Some weights of CanineForTokenClassification were not initialized from the model checkpoint at google/canine-c and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : Espagnol version ¤¤¤¤¤\n",
            "Reusing dataset conll2002 (/root/.cache/huggingface/datasets/conll2002/es/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5)\n",
            "100% 3/3 [00:00<00:00, 657.76it/s]\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : Dutch version ¤¤¤¤¤\n",
            "Reusing dataset conll2002 (/root/.cache/huggingface/datasets/conll2002/nl/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5)\n",
            "100% 3/3 [00:00<00:00, 831.60it/s]\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : English version ¤¤¤¤¤\n",
            "Reusing dataset conll2003 (/root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee)\n",
            "100% 3/3 [00:00<00:00, 654.10it/s]\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ Start training with NL  dataset ¤¤¤¤¤\n",
            " 10% 1579/15806 [05:19<44:58,  5.27it/s]\n",
            "Iteration 1580/15806 of epoch 1 complete. Loss : 0.019957318902015686\n",
            " 20% 3159/15806 [10:39<41:10,  5.12it/s]\n",
            "Iteration 3160/15806 of epoch 1 complete. Loss : 0.00016211035836022347\n",
            " 30% 4739/15806 [15:59<37:22,  4.93it/s]\n",
            "Iteration 4740/15806 of epoch 1 complete. Loss : 7.585813727928326e-05\n",
            " 40% 6319/15806 [21:18<33:37,  4.70it/s]\n",
            "Iteration 6320/15806 of epoch 1 complete. Loss : 0.01036966871470213\n",
            " 50% 7899/15806 [26:38<28:53,  4.56it/s]\n",
            "Iteration 7900/15806 of epoch 1 complete. Loss : 0.0003394854720681906\n",
            " 60% 9479/15806 [31:58<23:24,  4.50it/s]\n",
            "Iteration 9480/15806 of epoch 1 complete. Loss : 0.054476890712976456\n",
            " 70% 11059/15806 [37:19<17:20,  4.56it/s]\n",
            "Iteration 11060/15806 of epoch 1 complete. Loss : 0.05003553256392479\n",
            " 80% 12639/15806 [42:38<10:26,  5.05it/s]\n",
            "Iteration 12640/15806 of epoch 1 complete. Loss : 0.014872110448777676\n",
            " 90% 14219/15806 [47:58<04:50,  5.46it/s]\n",
            "Iteration 14220/15806 of epoch 1 complete. Loss : 0.0006911262753419578\n",
            "100% 15799/15806 [53:17<00:01,  5.05it/s]\n",
            "Iteration 15800/15806 of epoch 1 complete. Loss : 0.005942756775766611\n",
            "100% 15806/15806 [53:19<00:00,  4.94it/s]\n",
            "100% 2895/2895 [02:25<00:00, 19.88it/s]\n",
            "\n",
            "Epoch 1 complete! Validation Loss : 0.011728755588701375\n",
            "Best validation loss improved from inf to 0.011728755588701375\n",
            "\n",
            " 10% 1579/15806 [05:19<44:54,  5.28it/s]\n",
            "Iteration 1580/15806 of epoch 2 complete. Loss : 0.0045552789233624935\n",
            " 20% 3159/15806 [10:39<42:11,  5.00it/s]\n",
            "Iteration 3160/15806 of epoch 2 complete. Loss : 0.0002485605364199728\n",
            " 30% 4739/15806 [15:58<40:30,  4.55it/s]\n",
            "Iteration 4740/15806 of epoch 2 complete. Loss : 2.1972253307467327e-05\n",
            " 40% 6319/15806 [21:19<37:42,  4.19it/s]\n",
            "Iteration 6320/15806 of epoch 2 complete. Loss : 0.005195541772991419\n",
            " 50% 7899/15806 [26:39<29:33,  4.46it/s]\n",
            "Iteration 7900/15806 of epoch 2 complete. Loss : 3.2794250728329644e-05\n",
            " 60% 9479/15806 [31:59<24:41,  4.27it/s]\n",
            "Iteration 9480/15806 of epoch 2 complete. Loss : 0.05445066839456558\n",
            " 70% 11059/15806 [37:16<14:22,  5.50it/s]\n",
            "Iteration 11060/15806 of epoch 2 complete. Loss : 0.050038792192935944\n",
            " 80% 12639/15806 [42:36<09:44,  5.42it/s]\n",
            "Iteration 12640/15806 of epoch 2 complete. Loss : 0.021219804883003235\n",
            " 90% 14219/15806 [47:55<04:59,  5.30it/s]\n",
            "Iteration 14220/15806 of epoch 2 complete. Loss : 0.0006690490990877151\n",
            "100% 15799/15806 [53:14<00:01,  5.13it/s]\n",
            "Iteration 15800/15806 of epoch 2 complete. Loss : 0.0070218052715063095\n",
            "100% 15806/15806 [53:16<00:00,  4.95it/s]\n",
            "100% 2895/2895 [02:25<00:00, 19.83it/s]\n",
            "\n",
            "Epoch 2 complete! Validation Loss : 0.011880679052560386\n",
            " 10% 1579/15806 [05:19<44:17,  5.35it/s]\n",
            "Iteration 1580/15806 of epoch 3 complete. Loss : 0.003845816943794489\n",
            " 20% 3159/15806 [10:39<46:31,  4.53it/s]\n",
            "Iteration 3160/15806 of epoch 3 complete. Loss : 0.00016969081480056047\n",
            " 30% 4739/15806 [15:59<36:13,  5.09it/s]\n",
            "Iteration 4740/15806 of epoch 3 complete. Loss : 3.290560198365711e-05\n",
            " 40% 6319/15806 [21:18<31:05,  5.09it/s]\n",
            "Iteration 6320/15806 of epoch 3 complete. Loss : 0.001827102736569941\n",
            " 50% 7899/15806 [26:38<30:32,  4.32it/s]\n",
            "Iteration 7900/15806 of epoch 3 complete. Loss : 2.3337966922554187e-05\n",
            " 60% 9479/15806 [31:57<23:58,  4.40it/s]\n",
            "Iteration 9480/15806 of epoch 3 complete. Loss : 0.05230749770998955\n",
            " 70% 11059/15806 [37:15<15:20,  5.16it/s]\n",
            "Iteration 11060/15806 of epoch 3 complete. Loss : 0.05606064572930336\n",
            " 80% 12639/15806 [42:35<10:26,  5.05it/s]\n",
            "Iteration 12640/15806 of epoch 3 complete. Loss : 0.009304443374276161\n",
            " 90% 14219/15806 [47:55<05:07,  5.16it/s]\n",
            "Iteration 14220/15806 of epoch 3 complete. Loss : 0.00048725653323344886\n",
            "100% 15799/15806 [53:13<00:01,  5.02it/s]\n",
            "Iteration 15800/15806 of epoch 3 complete. Loss : 0.005056470632553101\n",
            "100% 15806/15806 [53:15<00:00,  4.95it/s]\n",
            "100% 2895/2895 [02:25<00:00, 19.94it/s]\n",
            "\n",
            "Epoch 3 complete! Validation Loss : 0.01044351893576009\n",
            "Best validation loss improved from 0.011728755588701375 to 0.01044351893576009\n",
            "\n",
            " 10% 1579/15806 [05:20<47:34,  4.98it/s]\n",
            "Iteration 1580/15806 of epoch 4 complete. Loss : 0.003382582450285554\n",
            " 20% 3159/15806 [10:39<41:34,  5.07it/s]\n",
            "Iteration 3160/15806 of epoch 4 complete. Loss : 2.9442793675116263e-05\n",
            " 30% 4739/15806 [15:57<33:40,  5.48it/s]\n",
            "Iteration 4740/15806 of epoch 4 complete. Loss : 2.1415007722680457e-05\n",
            " 40% 6319/15806 [21:16<31:40,  4.99it/s]\n",
            "Iteration 6320/15806 of epoch 4 complete. Loss : 0.0007729521603323519\n",
            " 50% 7899/15806 [26:35<25:41,  5.13it/s]\n",
            "Iteration 7900/15806 of epoch 4 complete. Loss : 2.3531913029728457e-05\n",
            " 60% 9479/15806 [31:53<21:35,  4.89it/s]\n",
            "Iteration 9480/15806 of epoch 4 complete. Loss : 0.04316248744726181\n",
            " 70% 11059/15806 [37:11<15:45,  5.02it/s]\n",
            "Iteration 11060/15806 of epoch 4 complete. Loss : 0.04819494113326073\n",
            " 80% 12639/15806 [42:30<12:39,  4.17it/s]\n",
            "Iteration 12640/15806 of epoch 4 complete. Loss : 0.004567037336528301\n",
            " 90% 14219/15806 [47:49<06:13,  4.25it/s]\n",
            "Iteration 14220/15806 of epoch 4 complete. Loss : 8.195320697268471e-05\n",
            "100% 15799/15806 [53:09<00:01,  5.04it/s]\n",
            "Iteration 15800/15806 of epoch 4 complete. Loss : 0.0027053887024521828\n",
            "100% 15806/15806 [53:11<00:00,  4.95it/s]\n",
            "100% 2895/2895 [02:25<00:00, 19.84it/s]\n",
            "\n",
            "Epoch 4 complete! Validation Loss : 0.008950872167564157\n",
            "Best validation loss improved from 0.01044351893576009 to 0.008950872167564157\n",
            "\n",
            "The model has been saved in /content/drive/MyDrive/ENS/NLP/models/CANINE_lr_2e-05_val_loss_0.00895_ep_4.pt\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ Start evaluation with  NL  dataset ¤¤¤¤¤\n",
            "100% 5.20k/5.20k [06:20<00:00, 13.7it/s, F1_score=0.945]\n",
            "final F1 score is :  0.9450180751896379\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 2 : Transfert learning CANINE vs BERT\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-B9VCbkCqp8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 main.py --model_type CANINE --canine_mode c --mode FULL --epochs 4 --train_lang EN --eval_lang ES"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JncOPWqWqpk1",
        "outputId": "b8a5e6a0-8119-4eef-e5cd-bcf084193935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ Creating model  CANINE ¤¤¤¤¤\n",
            "Some weights of CanineForTokenClassification were not initialized from the model checkpoint at google/canine-c and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : Espagnol version ¤¤¤¤¤\n",
            "Reusing dataset conll2002 (/root/.cache/huggingface/datasets/conll2002/es/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5)\n",
            "100% 3/3 [00:00<00:00, 645.67it/s]\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : Dutch version ¤¤¤¤¤\n",
            "Reusing dataset conll2002 (/root/.cache/huggingface/datasets/conll2002/nl/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5)\n",
            "100% 3/3 [00:00<00:00, 781.30it/s]\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : English version ¤¤¤¤¤\n",
            "Reusing dataset conll2003 (/root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee)\n",
            "100% 3/3 [00:00<00:00, 615.12it/s]\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ Start training with ES  dataset ¤¤¤¤¤\n",
            " 10% 1403/14041 [04:44<46:28,  4.53it/s]\n",
            "Iteration 1404/14041 of epoch 1 complete. Loss : 0.13295526802539825\n",
            " 20% 2807/14041 [09:35<39:54,  4.69it/s]\n",
            "Iteration 2808/14041 of epoch 1 complete. Loss : 0.005963237956166267\n",
            " 30% 4211/14041 [14:34<32:45,  5.00it/s]\n",
            "Iteration 4212/14041 of epoch 1 complete. Loss : 0.039824843406677246\n",
            " 40% 5615/14041 [19:29<25:22,  5.54it/s]\n",
            "Iteration 5616/14041 of epoch 1 complete. Loss : 0.010055162943899632\n",
            " 50% 7019/14041 [24:26<26:36,  4.40it/s]\n",
            "Iteration 7020/14041 of epoch 1 complete. Loss : 0.026026234030723572\n",
            " 60% 8423/14041 [29:25<20:08,  4.65it/s]\n",
            "Iteration 8424/14041 of epoch 1 complete. Loss : 0.016137972474098206\n",
            " 70% 9827/14041 [34:21<13:09,  5.34it/s]\n",
            "Iteration 9828/14041 of epoch 1 complete. Loss : 0.058820828795433044\n",
            " 80% 11231/14041 [39:08<08:50,  5.29it/s]\n",
            "Iteration 11232/14041 of epoch 1 complete. Loss : 0.0006147510721348226\n",
            " 90% 12635/14041 [44:10<04:25,  5.29it/s]\n",
            "Iteration 12636/14041 of epoch 1 complete. Loss : 0.013155985623598099\n",
            "100% 14039/14041 [49:06<00:00,  4.69it/s]\n",
            "Iteration 14040/14041 of epoch 1 complete. Loss : 0.002532629296183586\n",
            "100% 14041/14041 [49:06<00:00,  4.76it/s]\n",
            "100% 3250/3250 [02:57<00:00, 18.29it/s]\n",
            "\n",
            "Epoch 1 complete! Validation Loss : 0.01853877586697574\n",
            "Best validation loss improved from inf to 0.01853877586697574\n",
            "\n",
            " 10% 1403/14041 [04:46<40:41,  5.18it/s]\n",
            "Iteration 1404/14041 of epoch 2 complete. Loss : 0.08637621998786926\n",
            " 20% 2807/14041 [09:42<39:50,  4.70it/s]\n",
            "Iteration 2808/14041 of epoch 2 complete. Loss : 0.00397240836173296\n",
            " 30% 4211/14041 [14:35<30:54,  5.30it/s]\n",
            "Iteration 4212/14041 of epoch 2 complete. Loss : 0.028598183766007423\n",
            " 40% 5615/14041 [19:35<35:42,  3.93it/s]\n",
            "Iteration 5616/14041 of epoch 2 complete. Loss : 0.0027878470718860626\n",
            " 50% 7019/14041 [24:31<24:57,  4.69it/s]\n",
            "Iteration 7020/14041 of epoch 2 complete. Loss : 0.008918904699385166\n",
            " 60% 8423/14041 [29:34<19:50,  4.72it/s]\n",
            "Iteration 8424/14041 of epoch 2 complete. Loss : 0.007710954640060663\n",
            " 70% 9827/14041 [34:27<16:13,  4.33it/s]\n",
            "Iteration 9828/14041 of epoch 2 complete. Loss : 0.0638531818985939\n",
            " 80% 11231/14041 [39:26<09:54,  4.73it/s]\n",
            "Iteration 11232/14041 of epoch 2 complete. Loss : 6.9471076130867e-05\n",
            " 90% 12635/14041 [44:25<06:00,  3.90it/s]\n",
            "Iteration 12636/14041 of epoch 2 complete. Loss : 0.006475477479398251\n",
            "100% 14039/14041 [49:24<00:00,  5.18it/s]\n",
            "Iteration 14040/14041 of epoch 2 complete. Loss : 0.0023365309461951256\n",
            "100% 14041/14041 [49:24<00:00,  4.74it/s]\n",
            "100% 3250/3250 [02:59<00:00, 18.07it/s]\n",
            "\n",
            "Epoch 2 complete! Validation Loss : 0.014494112380245427\n",
            "Best validation loss improved from 0.01853877586697574 to 0.014494112380245427\n",
            "\n",
            " 10% 1403/14041 [05:01<42:28,  4.96it/s]\n",
            "Iteration 1404/14041 of epoch 3 complete. Loss : 0.072051040828228\n",
            " 20% 2807/14041 [09:55<42:24,  4.42it/s]\n",
            "Iteration 2808/14041 of epoch 3 complete. Loss : 0.003857346484437585\n",
            " 30% 4211/14041 [14:44<33:21,  4.91it/s]\n",
            "Iteration 4212/14041 of epoch 3 complete. Loss : 0.024945886805653572\n",
            " 40% 5615/14041 [19:35<29:32,  4.75it/s]\n",
            "Iteration 5616/14041 of epoch 3 complete. Loss : 0.0006437565898522735\n",
            " 50% 7019/14041 [24:34<21:17,  5.50it/s]\n",
            "Iteration 7020/14041 of epoch 3 complete. Loss : 0.0015379198594018817\n",
            " 60% 8423/14041 [29:22<19:34,  4.78it/s]\n",
            "Iteration 8424/14041 of epoch 3 complete. Loss : 0.005879566073417664\n",
            " 70% 9827/14041 [34:16<15:07,  4.65it/s]\n",
            "Iteration 9828/14041 of epoch 3 complete. Loss : 0.054113034158945084\n",
            " 80% 11231/14041 [39:05<11:00,  4.25it/s]\n",
            "Iteration 11232/14041 of epoch 3 complete. Loss : 1.5707153579569422e-05\n",
            " 90% 12635/14041 [43:59<06:22,  3.67it/s]\n",
            "Iteration 12636/14041 of epoch 3 complete. Loss : 0.004136655479669571\n",
            "100% 14039/14041 [48:50<00:00,  4.76it/s]\n",
            "Iteration 14040/14041 of epoch 3 complete. Loss : 0.001205821637995541\n",
            "100% 14041/14041 [48:50<00:00,  4.79it/s]\n",
            "100% 3250/3250 [02:57<00:00, 18.26it/s]\n",
            "\n",
            "Epoch 3 complete! Validation Loss : 0.011841302422472276\n",
            "Best validation loss improved from 0.014494112380245427 to 0.011841302422472276\n",
            "\n",
            " 10% 1403/14041 [04:52<40:22,  5.22it/s]\n",
            "Iteration 1404/14041 of epoch 4 complete. Loss : 0.05899130180478096\n",
            " 20% 2807/14041 [09:33<40:29,  4.62it/s]\n",
            "Iteration 2808/14041 of epoch 4 complete. Loss : 0.003241888480260968\n",
            " 30% 4211/14041 [14:26<30:12,  5.42it/s]\n",
            "Iteration 4212/14041 of epoch 4 complete. Loss : 0.019175581634044647\n",
            " 40% 5615/14041 [19:25<30:44,  4.57it/s]\n",
            "Iteration 5616/14041 of epoch 4 complete. Loss : 0.000297289079753682\n",
            " 50% 7019/14041 [24:12<21:03,  5.56it/s]\n",
            "Iteration 7020/14041 of epoch 4 complete. Loss : 0.0004745736951008439\n",
            " 60% 8423/14041 [29:08<17:58,  5.21it/s]\n",
            "Iteration 8424/14041 of epoch 4 complete. Loss : 0.0015357962111011147\n",
            " 70% 9827/14041 [34:14<16:41,  4.21it/s]\n",
            "Iteration 9828/14041 of epoch 4 complete. Loss : 0.042765017598867416\n",
            " 80% 11231/14041 [39:15<08:52,  5.28it/s]\n",
            "Iteration 11232/14041 of epoch 4 complete. Loss : 8.675056960782968e-06\n",
            " 90% 12635/14041 [44:18<04:57,  4.73it/s]\n",
            "Iteration 12636/14041 of epoch 4 complete. Loss : 0.003988433163613081\n",
            "100% 14039/14041 [49:23<00:00,  4.00it/s]\n",
            "Iteration 14040/14041 of epoch 4 complete. Loss : 9.94915098999627e-05\n",
            "100% 14041/14041 [49:23<00:00,  4.74it/s]\n",
            "100% 3250/3250 [03:03<00:00, 17.71it/s]\n",
            "\n",
            "Epoch 4 complete! Validation Loss : 0.010349333795305343\n",
            "Best validation loss improved from 0.011841302422472276 to 0.010349333795305343\n",
            "\n",
            "The model has been saved in /content/drive/MyDrive/ENS/NLP/models/CANINE_lr_2e-05_val_loss_0.01035_ep_4.pt\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ Start evaluation with  ES  dataset ¤¤¤¤¤\n",
            "100% 1.52k/1.52k [01:26<00:00, 17.4it/s, F1_score=0.911]\n",
            "final F1 score is :  0.9105369515645861\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 main.py --model_type BERT --mode EVAL --eval_lang ES"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GE5YNt4JrfpM",
        "outputId": "ecd85c46-7839-47b1-f6e6-02ef84614e80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ Creating model  BERT ¤¤¤¤¤\n",
            "¤¤¤¤¤ only evalutation in EN mode is available ¤¤¤¤¤\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : Espagnol version ¤¤¤¤¤\n",
            "Reusing dataset conll2002 (/root/.cache/huggingface/datasets/conll2002/es/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5)\n",
            "100% 3/3 [00:00<00:00, 664.71it/s]\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Maximum token length for BERT is set to 512\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : Dutch version ¤¤¤¤¤\n",
            "Reusing dataset conll2002 (/root/.cache/huggingface/datasets/conll2002/nl/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5)\n",
            "100% 3/3 [00:00<00:00, 842.00it/s]\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : English version ¤¤¤¤¤\n",
            "Reusing dataset conll2003 (/root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee)\n",
            "100% 3/3 [00:00<00:00, 661.04it/s]\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ Start evaluation with  ES  dataset ¤¤¤¤¤\n",
            "100% 1.52k/1.52k [02:09<00:00, 11.7it/s, F1_score=0.846]\n",
            "final F1 score is :  0.8463608494151036\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 main.py --model_type CANINE --canine_mode c --mode EVAL --eval_lang NL --weight_name CANINE-C_EN_lr_2e-05_val_loss_0.01035_ep_4.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3xUMuGgxesO",
        "outputId": "e1fa802b-ad1b-465b-cf27-b764bf4342f7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ Creating model  CANINE ¤¤¤¤¤\n",
            "Downloading: 100% 698/698 [00:00<00:00, 681kB/s]\n",
            "Downloading: 100% 504M/504M [00:08<00:00, 58.7MB/s]\n",
            "Some weights of CanineForTokenClassification were not initialized from the model checkpoint at google/canine-c and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : Espagnol version ¤¤¤¤¤\n",
            "Downloading builder script: 9.23kB [00:00, 8.44MB/s]       \n",
            "Downloading metadata: 7.46kB [00:00, 8.53MB/s]       \n",
            "Downloading and preparing dataset conll2002/es (download: 3.95 MiB, generated: 8.87 MiB, post-processed: Unknown size, total: 12.82 MiB) to /root/.cache/huggingface/datasets/conll2002/es/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5...\n",
            "Downloading data files:   0% 0/3 [00:00<?, ?it/s]\n",
            "Downloading data: 2.97MB [00:00, 50.7MB/s]      \n",
            "Downloading data files:  33% 1/3 [00:00<00:01,  1.33it/s]\n",
            "Downloading data: 594kB [00:00, 21.2MB/s]       \n",
            "Downloading data files:  67% 2/3 [00:01<00:00,  1.15it/s]\n",
            "Downloading data: 576kB [00:00, 21.7MB/s]       \n",
            "Downloading data files: 100% 3/3 [00:02<00:00,  1.33it/s]\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 1681.53it/s]\n",
            "Dataset conll2002 downloaded and prepared to /root/.cache/huggingface/datasets/conll2002/es/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 357.55it/s]\n",
            "Downloading: 100% 657/657 [00:00<00:00, 558kB/s]\n",
            "Downloading: 100% 892/892 [00:00<00:00, 856kB/s]\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : Dutch version ¤¤¤¤¤\n",
            "Downloading and preparing dataset conll2002/nl (download: 3.47 MiB, generated: 7.74 MiB, post-processed: Unknown size, total: 11.21 MiB) to /root/.cache/huggingface/datasets/conll2002/nl/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5...\n",
            "Downloading data files:   0% 0/3 [00:00<?, ?it/s]\n",
            "Downloading data: 2.38MB [00:00, 46.7MB/s]      \n",
            "Downloading data files:  33% 1/3 [00:00<00:01,  1.54it/s]\n",
            "Downloading data: 451kB [00:00, 17.4MB/s]       \n",
            "Downloading data files:  67% 2/3 [00:01<00:00,  1.74it/s]\n",
            "Downloading data: 814kB [00:00, 26.0MB/s]       \n",
            "Downloading data files: 100% 3/3 [00:01<00:00,  1.75it/s]\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 1643.97it/s]\n",
            "Dataset conll2002 downloaded and prepared to /root/.cache/huggingface/datasets/conll2002/nl/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 820.96it/s]\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : English version ¤¤¤¤¤\n",
            "Downloading builder script: 9.52kB [00:00, 8.73MB/s]       \n",
            "Downloading metadata: 3.79kB [00:00, 4.34MB/s]       \n",
            "Downloading and preparing dataset conll2003/conll2003 (download: 959.94 KiB, generated: 9.78 MiB, post-processed: Unknown size, total: 10.72 MiB) to /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee...\n",
            "Downloading data: 100% 983k/983k [00:01<00:00, 753kB/s]\n",
            "Dataset conll2003 downloaded and prepared to /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 648.84it/s]\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CANINE pretrained weights on NER task ¤¤¤¤¤\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ Start evaluation with  NL  dataset ¤¤¤¤¤\n",
            "100% 5.20k/5.20k [06:09<00:00, 14.1it/s, F1_score=0.911]\n",
            "final F1 score is :  0.910985234156661\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 main.py --model_type CANINE --canine_mode c --mode EVAL --eval_lang NL --weight_name CANINE-C_ES_lr_2e-05_val_loss_0.01858_ep_4.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVw_MLYRysF1",
        "outputId": "2bdc493f-2a98-491d-8250-860d403cf0dd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ Creating model  CANINE ¤¤¤¤¤\n",
            "Some weights of CanineForTokenClassification were not initialized from the model checkpoint at google/canine-c and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : Espagnol version ¤¤¤¤¤\n",
            "Reusing dataset conll2002 (/root/.cache/huggingface/datasets/conll2002/es/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5)\n",
            "100% 3/3 [00:00<00:00, 644.72it/s]\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : Dutch version ¤¤¤¤¤\n",
            "Reusing dataset conll2002 (/root/.cache/huggingface/datasets/conll2002/nl/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5)\n",
            "100% 3/3 [00:00<00:00, 793.52it/s]\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : English version ¤¤¤¤¤\n",
            "Reusing dataset conll2003 (/root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee)\n",
            "100% 3/3 [00:00<00:00, 650.21it/s]\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CANINE pretrained weights on NER task ¤¤¤¤¤\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ Start evaluation with  NL  dataset ¤¤¤¤¤\n",
            "100% 5.20k/5.20k [06:16<00:00, 13.8it/s, F1_score=0.86]\n",
            "final F1 score is :  0.859669306293839\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 main.py --model_type CANINE --canine_mode c --mode EVAL --eval_lang EN --weight_name CANINE-C_ES_lr_2e-05_val_loss_0.01858_ep_4.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "247yiWCoztTA",
        "outputId": "5475e7f6-1642-46eb-993a-7e1ddbaef9c3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ Creating model  CANINE ¤¤¤¤¤\n",
            "Some weights of CanineForTokenClassification were not initialized from the model checkpoint at google/canine-c and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : Espagnol version ¤¤¤¤¤\n",
            "Reusing dataset conll2002 (/root/.cache/huggingface/datasets/conll2002/es/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5)\n",
            "100% 3/3 [00:00<00:00, 658.41it/s]\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : Dutch version ¤¤¤¤¤\n",
            "Reusing dataset conll2002 (/root/.cache/huggingface/datasets/conll2002/nl/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5)\n",
            "100% 3/3 [00:00<00:00, 696.84it/s]\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : English version ¤¤¤¤¤\n",
            "Reusing dataset conll2003 (/root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee)\n",
            "100% 3/3 [00:00<00:00, 653.45it/s]\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CANINE pretrained weights on NER task ¤¤¤¤¤\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ Start evaluation with  EN  dataset ¤¤¤¤¤\n",
            "100% 3.45k/3.45k [03:16<00:00, 17.6it/s, F1_score=0.774]\n",
            "final F1 score is :  0.7743400546580566\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 main.py --model_type CANINE --canine_mode c --mode EVAL --eval_lang ES --weight_name CANINE-C_NL_lr_2e-05_val_loss_0.00897_ep_4.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OWK5ZykcPej",
        "outputId": "f5ea9e77-b247-4feb-cc16-cc2f04af985c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ Creating model  CANINE ¤¤¤¤¤\n",
            "Some weights of CanineForTokenClassification were not initialized from the model checkpoint at google/canine-c and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : Espagnol version ¤¤¤¤¤\n",
            "Reusing dataset conll2002 (/root/.cache/huggingface/datasets/conll2002/es/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5)\n",
            "100% 3/3 [00:00<00:00, 527.01it/s]\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : Dutch version ¤¤¤¤¤\n",
            "Reusing dataset conll2002 (/root/.cache/huggingface/datasets/conll2002/nl/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5)\n",
            "100% 3/3 [00:00<00:00, 677.08it/s]\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : English version ¤¤¤¤¤\n",
            "Reusing dataset conll2003 (/root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee)\n",
            "100% 3/3 [00:00<00:00, 503.90it/s]\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CANINE pretrained weights on NER task ¤¤¤¤¤\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ Start evaluation with  ES  dataset ¤¤¤¤¤\n",
            "100% 1.52k/1.52k [01:41<00:00, 15.0it/s, F1_score=0.886]\n",
            "final F1 score is :  0.8860132771718285\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 3 : CANINE vs BERT"
      ],
      "metadata": {
        "id": "-fgvYGTGtjiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 main.py --model_type CANINE --canine_mode c --mode EVAL --eval_lang EN --weight_name CANINE-C_EN_lr_2e-05_val_loss_0.01035_ep_4.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5oj_9k3thRh",
        "outputId": "19855cfc-5681-4b27-9f4b-11c10b33f1fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ Creating model  CANINE ¤¤¤¤¤\n",
            "Downloading: 100% 698/698 [00:00<00:00, 752kB/s]\n",
            "Downloading: 100% 504M/504M [00:07<00:00, 72.1MB/s]\n",
            "Some weights of CanineForTokenClassification were not initialized from the model checkpoint at google/canine-c and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : Espagnol version ¤¤¤¤¤\n",
            "Downloading builder script: 9.23kB [00:00, 9.29MB/s]       \n",
            "Downloading metadata: 7.46kB [00:00, 7.96MB/s]       \n",
            "Downloading and preparing dataset conll2002/es (download: 3.95 MiB, generated: 8.87 MiB, post-processed: Unknown size, total: 12.82 MiB) to /root/.cache/huggingface/datasets/conll2002/es/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5...\n",
            "Downloading data files:   0% 0/3 [00:00<?, ?it/s]\n",
            "Downloading data: 2.97MB [00:00, 80.7MB/s]      \n",
            "Downloading data files:  33% 1/3 [00:01<00:02,  1.15s/it]\n",
            "Downloading data: 594kB [00:00, 42.4MB/s]       \n",
            "Downloading data files:  67% 2/3 [00:01<00:00,  1.19it/s]\n",
            "Downloading data: 576kB [00:00, 46.1MB/s]       \n",
            "Downloading data files: 100% 3/3 [00:02<00:00,  1.42it/s]\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 1680.41it/s]\n",
            "Dataset conll2002 downloaded and prepared to /root/.cache/huggingface/datasets/conll2002/es/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 353.96it/s]\n",
            "Downloading: 100% 657/657 [00:00<00:00, 905kB/s]\n",
            "Downloading: 100% 892/892 [00:00<00:00, 1.47MB/s]\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : Dutch version ¤¤¤¤¤\n",
            "Downloading and preparing dataset conll2002/nl (download: 3.47 MiB, generated: 7.74 MiB, post-processed: Unknown size, total: 11.21 MiB) to /root/.cache/huggingface/datasets/conll2002/nl/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5...\n",
            "Downloading data files:   0% 0/3 [00:00<?, ?it/s]\n",
            "Downloading data: 2.38MB [00:00, 69.0MB/s]      \n",
            "Downloading data files:  33% 1/3 [00:00<00:01,  1.15it/s]\n",
            "Downloading data: 451kB [00:00, 26.7MB/s]       \n",
            "Downloading data files:  67% 2/3 [00:01<00:00,  1.84it/s]\n",
            "Downloading data: 814kB [00:00, 47.6MB/s]       \n",
            "Downloading data files: 100% 3/3 [00:01<00:00,  1.69it/s]\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 1750.79it/s]\n",
            "Dataset conll2002 downloaded and prepared to /root/.cache/huggingface/datasets/conll2002/nl/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 851.69it/s]\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : English version ¤¤¤¤¤\n",
            "Downloading builder script: 9.52kB [00:00, 11.1MB/s]       \n",
            "Downloading metadata: 3.79kB [00:00, 5.91MB/s]       \n",
            "Downloading and preparing dataset conll2003/conll2003 (download: 959.94 KiB, generated: 9.78 MiB, post-processed: Unknown size, total: 10.72 MiB) to /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee...\n",
            "Downloading data: 100% 983k/983k [00:00<00:00, 1.38MB/s]\n",
            "Dataset conll2003 downloaded and prepared to /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 637.43it/s]\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CANINE pretrained weights on NER task ¤¤¤¤¤\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ Start evaluation with  EN  dataset ¤¤¤¤¤\n",
            "100% 3.45k/3.45k [03:09<00:00, 18.3it/s, F1_score=0.914]\n",
            "final F1 score is :  0.9139266548859056\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 main.py --model_type BERT --mode EVAL --eval_lang EN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR_8axpitxVa",
        "outputId": "9cdafdc5-0f97-4058-9b30-b7a16dc7f93b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ Creating model  BERT ¤¤¤¤¤\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : Espagnol version ¤¤¤¤¤\n",
            "Reusing dataset conll2002 (/root/.cache/huggingface/datasets/conll2002/es/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5)\n",
            "100% 3/3 [00:00<00:00, 565.02it/s]\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : Dutch version ¤¤¤¤¤\n",
            "Reusing dataset conll2002 (/root/.cache/huggingface/datasets/conll2002/nl/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5)\n",
            "100% 3/3 [00:00<00:00, 814.59it/s]\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ loading CONLL2002 dataset : English version ¤¤¤¤¤\n",
            "Reusing dataset conll2003 (/root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee)\n",
            "100% 3/3 [00:00<00:00, 619.54it/s]\n",
            "Maximum token length for BERT is set to 512\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n",
            "¤¤¤¤¤ Start evaluation with  EN  dataset ¤¤¤¤¤\n",
            "100% 3.45k/3.45k [05:01<00:00, 11.5it/s, F1_score=0.756]\n",
            "final F1 score is :  0.7561281204520202\n",
            "¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤\n"
          ]
        }
      ]
    }
  ]
}